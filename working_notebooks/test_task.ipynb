{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57521e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c4982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe87faf",
   "metadata": {},
   "source": [
    "# OK let's first see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_show_img(filepath):\n",
    "    plt.figure(figsize = (30,30))\n",
    "    image = cv2.imread(filepath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987873db",
   "metadata": {},
   "outputs": [],
   "source": [
    "uav = \"dataset/RGB/uav_images/\"\n",
    "sat = \"dataset/RGB/sat_images/\"\n",
    "fsat = \"dataset/RGB/false_sat_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_show_img(uav + \"DJI_0669.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_show_img(sat+\"DJI_0669.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_show_img(fsat+\"(38.5727858867262,-90.1775736641245), (38.57184403801162, -90.17577406541481).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/RGB/metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f28b3",
   "metadata": {},
   "source": [
    "## Ok, so here I assume that images from sat and uav dirs with the same name are images of the same location that should be matched, while false sat images are random locations to check for false positives when matching.\n",
    "Also, for now I'll mostly ignore metadata, though not yet sure if there's something useful for the task, like the information on season etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb1727",
   "metadata": {},
   "source": [
    "# Initial plan\n",
    "<ol>\n",
    "  <li>Build simple dataset with train/validation/test split, with pairs of images, and values 0 if it's different location and 1 if the same location </li>\n",
    "    <li>Create simple baseline solution</li>\n",
    "    <li/> Evaluate\n",
    "    <li/>See how good it is, what's bad and what to do next\n",
    "</ol>\n",
    "\n",
    "For reproducability let's set random seed constant for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a20b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6588807",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names_sat = np.array(os.listdir(sat))\n",
    "correct_names_uav = np.array(os.listdir(uav))\n",
    "false_names = np.array(os.listdir(fsat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aec462",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(correct_names_sat) == set(correct_names_uav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a934e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_names_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22fc89",
   "metadata": {},
   "source": [
    "Ok so balance is around 1/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8246cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrain, ctest = train_test_split(correct_names_sat, test_size = 0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain, ftest = train_test_split(false_names, test_size = 0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d417376",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrain.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11fa52",
   "metadata": {},
   "source": [
    "Now, let's define what is correct and what is not correct\n",
    "So far, if img has same name it represents same location, even if in different folders\n",
    "And if the img has different name, even if in the same folder, it's different location\n",
    "Then types of pairs can be:\n",
    "<ol>\n",
    "    <li>identical images from any folder - this part I'll drop as I asumme it'll only pollute the dataset</li>\n",
    "    <li>pair of sat image and uav image with same name - correct pair with value of 1 </li>\n",
    "    <li>pair of sat image and uav image with a different name - incorrect pair with value of 0 </li>\n",
    "    <li>pair of sat image and false image - incorrect pair with value of 0 </li>\n",
    "    <li>pair of uav image and false image - incorrect pair with value of 0 </li>\n",
    "    <li>pair of two non-identical false images - incorrect pair with value of 0 </li>\n",
    "</ol>\n",
    "For initial version I'll aim to a split to approximately have 1 / 1 / 3 / 9, which would mean 1 to 13 balance of classes, which is somewhat similar to 1/16 image split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correct_set(arr):\n",
    "    return np.array([[uav + x, sat + x, 1] for x in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_two_arrays(arr):\n",
    "    r = np.array(arr, copy=True)  \n",
    "    l = np.array(arr, copy=True)\n",
    "    np.random.shuffle(l)\n",
    "    return r, l\n",
    "\n",
    "def create_incorrect_same_category(arr, pref1, pref2, limit = 10000):\n",
    "    r, l = make_two_arrays(arr)\n",
    "    same = []\n",
    "    res = []\n",
    "    #maybe it's a little inefficient, but it doesn't impact performance of final solution so should be fine\n",
    "    while len(r) > 1 or len(res) == 0 or len(res) >= limit:\n",
    "        for i in range(len(r)):\n",
    "            if r[i] != l[i]:\n",
    "                res.append([pref1 + r[i], pref2 + l[i], 0])\n",
    "                if len(res) == limit:\n",
    "                    return np.array(res)\n",
    "            else:\n",
    "                same.append(r[i])\n",
    "        r, l = make_two_arrays(same)\n",
    "        same = []\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10849d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = create_correct_set(ctrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12820d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect1 = create_incorrect_same_category(ctrain, uav, sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19513d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ec99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correct_false_pairs(cvals, fvals, scale_factor, pref1, pref2):\n",
    "    from_correct = np.array(list(cvals) * 3)\n",
    "    fvalues = np.array(list(fvals) * (int(len(from_correct) / len(fvals)) + 1))\n",
    "    #To make selection more smooth, same item can't be selected more than 1 time on the current dataset, so it forces to select different ones\n",
    "    random_choices = np.random.choice(fvalues, len(from_correct), replace = False)\n",
    "    return np.array([[pref1 + from_correct[i], pref2 + random_choices[i], 0] for i in range(len(random_choices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95598706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incorrect2 = create_correct_false_pairs(ctrain, ftrain, 3, uav, fsat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect3 = create_incorrect_same_category(ftrain, fsat, fsat, len(correct) * 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30984945",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_array = np.concatenate((correct, incorrect1, incorrect2, incorrect3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(resulting_array, columns = [\"path1\", \"path2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label == \"1\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1152e8",
   "metadata": {},
   "source": [
    "now I'll assemble it into a function, as I'll need 2 of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10941087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfSplit(carr, farr, scale_one_correct, scale_no_correct):\n",
    "    print(\"creating correct\")\n",
    "    correct = create_correct_set(carr)\n",
    "    print(\"correct_number \" + str(len(correct)))\n",
    "    print(\"creating incorrect1\")\n",
    "    incorrect1 = create_incorrect_same_category(carr, uav, sat)\n",
    "    print(\"icorrect1_number \" + str(len(incorrect1)))\n",
    "    print(\"creating incorrect2\")\n",
    "    incorrect2 = create_correct_false_pairs(carr, farr, scale_one_correct, uav, fsat)\n",
    "    print(\"icorrect2_number \" + str(len(incorrect2)))\n",
    "    print(\"creating incorrect3\")\n",
    "    incorrect3 = create_incorrect_same_category(farr, fsat, fsat, len(correct) * scale_no_correct)\n",
    "    print(\"icorrect3_number \" + str(len(incorrect3)))\n",
    "    print(\"Concatenating\")\n",
    "    resulting_array = np.concatenate((correct, incorrect1, incorrect2, incorrect3))\n",
    "    print(\"concatenated_number \" + str(len(resulting_array)))\n",
    "    return pd.DataFrame(resulting_array, columns = [\"path1\", \"path2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = getDfSplit(ctrain, ftrain, 3, 9)\n",
    "df_test = getDfSplit(ctest, ftest, 3, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ef019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c754d",
   "metadata": {},
   "source": [
    "### Now, that dataset is at least defined, we can proceed to actual algorithm \n",
    "For this, because images can be different in brightness, color, rotation, etc, we need some kind of feature extraction algorithm\n",
    "Basically, there are 2 ways, somewhat oldschool with known feature extractor like SIFT on both images and with some ML algorith compare them; and second way is to create some CNN architecture, like siamese networks, or to do both or multitude of one and ensemble. I'll rule out ensembling, as it tends to increase computing times significantly.\n",
    "Let's start with some CNN and see what we have from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6a35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db33315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "from torch import tensor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ff200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_dataset(path, dataloader):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        with open(path + str(i) + \".pkl\", 'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    with open(\"processed_dataset/\" + str(i) + \".pkl\", 'wb') as outp:\n",
    "        pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader_eval):\n",
    "    with open(\"processed_eval/\" + str(i) + \".pkl\", 'wb') as outp:\n",
    "        pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580e165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"processed_dataset/0.pkl\", 'rb') as inp:\n",
    "    company1 = pickle.load(inp)\n",
    "    print(company1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae70cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4d94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseResNet, self).__init__()\n",
    "        if torch.cuda.is_available(): \n",
    "            dev = \"cuda:0\" \n",
    "        else: \n",
    "            dev = \"cpu\" \n",
    "        self.dev = torch.device(dev) \n",
    "        # Load pre-trained ResNet-18 models\n",
    "        self.cnn1 = self.load_pretrained_model()\n",
    "        self.cnn2 = self.load_pretrained_model()\n",
    "        \n",
    "        for param in self.cnn1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.cnn2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add a new layer on top\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(25088 * 2),\n",
    "            nn.Linear(25088 * 2, 4096),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1),\n",
    "        )# Input size is 512 for ResNet-18\n",
    "        self.to(self.dev)\n",
    "        \n",
    "    def load_pretrained_model(self):\n",
    "        # Download and load a pre-trained ResNet-18 model\n",
    "        model = models.vgg16(weights=True)\n",
    "        return model.features\n",
    "        \n",
    "    def forward_once(self, x1, x2):\n",
    "        # Forward pass through both of the CNN branches\n",
    "        out1 = self.cnn1(x1)\n",
    "        out1 = out1.view(out1.size()[0], -1)\n",
    "        out2 = self.cnn2(x2)\n",
    "        out2 = out2.view(out2.size()[0], -1)\n",
    "        return out1, out2\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass through both CNN branches\n",
    "        output1, output2 = self.forward_once(input1.to(self.dev), input2.to(self.dev))\n",
    "        \n",
    "        # Concatenate the outputs of both branches\n",
    "        combined = torch.cat((output1, output2), dim=1)\n",
    "        #print(\"COMBINED\")\n",
    "        #print(combined)\n",
    "        # Forward pass through the new layer\n",
    "        output = self.classifier(combined)\n",
    "        #print(\"OUTPUT : \" + str(output))\n",
    "        #print(\"SIGMOID\")\n",
    "        #print(torch.sigmoid(output))\n",
    "        return torch.sigmoid(output)  # Apply sigmoid activation for binary classification\n",
    "\n",
    "# Instantiate the SiameseResNet model\n",
    "model = SiameseResNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.data[idx]\n",
    "        \n",
    "        # Load images using OpenCV\n",
    "        img1 = read_image(img1_path)\n",
    "        img2 = read_image(img2_path)\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        tens = torch.tensor(float(label))\n",
    "        return img1, img2, tens\n",
    "\n",
    "# Transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy arrays to PIL images\n",
    "    transforms.Resize((224, 224)),# Resize images to fit ResNet input size\n",
    "    transforms.ToTensor(),# Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image pixels\n",
    "])\n",
    "\n",
    "# Create a SiameseDataset instance\n",
    "dataset = SiameseDataset(df_train.values, transform=transform)\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "eval_set = SiameseDataset(df_test.values, transform=transform)\n",
    "dataloader_eval = DataLoader(eval_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82009dd5",
   "metadata": {},
   "source": [
    "Here I've noticed that 90% of the time learning is actually spent in preprocessing stage, so we might as well run it once, store results (which is approx 5gb, which might even fit into RAM or for 16 gb VRAM even into it, but I'll continue with this setup), and then just read already processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4092d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(path, num):\n",
    "    with open(path + str(num) + \".pkl\", 'rb') as inp:\n",
    "        data = pickle.load(inp)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader1, dataloader2):\n",
    "    loss1, acc1 = model_eval(model, dataloader1)\n",
    "    loss2, acc2 = model_eval(model, dataloader2)\n",
    "    return loss1, loss2, acc1, acc2\n",
    "\n",
    "def model_eval(model, dataloader):\n",
    "    total_correct = 0.\n",
    "    total_samples = 0.\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            img1, img2, labels = data\n",
    "            reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(img1, img2)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, reshaped_labels)\n",
    "            \n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct = (predictions == reshaped_labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss\n",
    "    return total_loss, float(total_correct)/total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_prep(model, criterion = nn.BCELoss(weight = tensor([13]).to(device))):\n",
    "    path_train = \"processed_dataset/\"\n",
    "    train_max_index = 148\n",
    "    path_test = \"processed_eval/\"\n",
    "    test_max_index = 37\n",
    "    loss1, acc1 = model_eval_prep(model, path_train, train_max_index, criterion)\n",
    "    loss2, acc2 = model_eval_prep(model, path_test, test_max_index, criterion)\n",
    "    return loss1, loss2, acc1, acc2\n",
    "\n",
    "def model_eval_prep(model, path, max_index, criterion):\n",
    "    total_correct = 0.\n",
    "    total_samples = 0.\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range (max_index + 1):\n",
    "            with open(path + str(i) + \".pkl\", 'rb') as inp:\n",
    "                data = pickle.load(inp)\n",
    "            img1, img2, labels = data\n",
    "            reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(img1, img2)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, reshaped_labels)\n",
    "            \n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct = (predictions == reshaped_labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss\n",
    "    return total_loss/total_samples, float(total_correct)/total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tensor([ 13.]).to(device) #\n",
    "criterion = nn.BCELoss(weight = weight) #Initially I used BCELoss but it appears to be bugged https://discuss.pytorch.org/t/model-weights-not-being-updated/1842/6\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 3\n",
    "path_train = \"processed_dataset/\"\n",
    "train_max_index = 148\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    loading_time_start = time.time()\n",
    "    \n",
    "    for i in range(train_max_index + 1):\n",
    "        with open(path_train + str(i) + \".pkl\", 'rb') as inp:\n",
    "            data = pickle.load(inp)\n",
    "        if i % 10 == 9:\n",
    "            print(\"loading time : \" + str(time.time() - loading_time_start))\n",
    "        mini_epoch_start = time.time()\n",
    "        img1, img2, labels = data\n",
    "        reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        value_before = float(list(model.parameters())[-1][0])\n",
    "        # Forward pass\n",
    "        outputs = model(img1, img2)\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, reshaped_labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        value_after = float(list(model.parameters())[-1][0])\n",
    "        if abs(value_before - value_after) < 0.000000000001:\n",
    "            print(\"VALUES ARE THE SAME\")\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 mini-batches\n",
    "            print('%f per batch : [%d, %5d] loss: %.3f' %\n",
    "                  (time.time() - mini_epoch_start, epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "        loading_time_start = time.time()\n",
    "    model.eval()\n",
    "    loss_train, loss_test, acc_train, acc_test = evaluate_model_prep(model)\n",
    "    print('\\n loss train: %.3f; acc train %.3f \\n loss test %.3f acc test %.3f' %\n",
    "                  (loss_train, acc_train * 100, loss_test, acc_test * 100))\n",
    "    model.train()\n",
    "    epoch_end = time.time()\n",
    "    print(\"EPOCH TIME : \" + str(epoch_end - epoch_start))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in enumerate(dataloader):\n",
    "    print(k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee658ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image(uav + \"DJI_0669.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65594e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12115e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_class = model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef16861",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_class[6] = nn.Linear(4096, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194362e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e088e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_epoch_start = time.time()\n",
    "k = 0\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(\"time to get next \" + str(time.time() - mini_epoch_start))\n",
    "    k+=1\n",
    "    if (k > 4):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model.parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fcdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(list(model.parameters())[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e26f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_train + str(1) + \".pkl\", 'rb') as inp:\n",
    "        data = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be489f",
   "metadata": {},
   "source": [
    "OK so time to evaluate it to some extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = []\n",
    "predict_labels = []\n",
    "model.eval()\n",
    "path = \"processed_eval/\"\n",
    "max_index = 37\n",
    "with torch.no_grad():\n",
    "    for i in range (max_index + 1):\n",
    "        with open(path + str(i) + \".pkl\", 'rb') as inp:\n",
    "            data = pickle.load(inp)\n",
    "        img1, img2, labels = data\n",
    "        correct_labels = correct_labels + labels.tolist()\n",
    "        reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(img1, img2)\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, reshaped_labels)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        predict_labels = predict_labels + predictions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(correct_labels, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93902085",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(correct_labels, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(correct_labels, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(correct_labels, predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6a132",
   "metadata": {},
   "source": [
    "So after initial experiment I have\n",
    "precision 0.5897435897435898\n",
    "recall 0.27058823529411763\n",
    "f1 0.3709677419354838\n",
    "So, I decided to repeat experiment, but now to upscale number of positive samples in the dataset and repeat learning. I'll keep same number of negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23981790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfSplitUpscaled(carr, farr, scale_one_correct, scale_no_correct):\n",
    "    print(\"creating correct\")\n",
    "    correct = create_correct_set(carr)\n",
    "    print(\"correct_number \" + str(len(correct)))\n",
    "    print(\"creating incorrect1\")\n",
    "    incorrect1 = create_incorrect_same_category(carr, uav, sat)\n",
    "    print(\"icorrect1_number \" + str(len(incorrect1)))\n",
    "    print(\"creating incorrect2\")\n",
    "    incorrect2 = create_correct_false_pairs(carr, farr, scale_one_correct, uav, fsat)\n",
    "    print(\"icorrect2_number \" + str(len(incorrect2)))\n",
    "    print(\"creating incorrect3\")\n",
    "    incorrect3 = create_incorrect_same_category(farr, fsat, fsat, len(correct) * scale_no_correct)\n",
    "    print(\"icorrect3_number \" + str(len(incorrect3)))\n",
    "    print(\"Concatenating\")\n",
    "    resulting_array = np.concatenate((correct, correct, correct, correct, incorrect1, incorrect2, incorrect3))\n",
    "    print(\"concatenated_number \" + str(len(resulting_array)))\n",
    "    return pd.DataFrame(resulting_array, columns = [\"path1\", \"path2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_upscaled = getDfSplitUpscaled(ctrain, ftrain, 3, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb352f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_upscaled[df_train_upscaled.label == \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68553987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SiameseDataset instance\n",
    "dataset_upscaled = SiameseDataset(df_train_upscaled.values, transform=transform)\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 32\n",
    "dataloader_upscaled = DataLoader(dataset_upscaled, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_dataset(\"processed_dataset/\", dataloader_upscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, train_max_index, path_train, weight_n, lr):\n",
    "    weight = tensor([weight_n]).to(device) #\n",
    "    criterion = nn.BCELoss(weight = weight) #Initially I used BCELoss but it appears to be bugged https://discuss.pytorch.org/t/model-weights-not-being-updated/1842/6\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "        loading_time_start = time.time()\n",
    "\n",
    "        for i in range(train_max_index + 1):\n",
    "            with open(path_train + str(i) + \".pkl\", 'rb') as inp:\n",
    "                data = pickle.load(inp)\n",
    "            if i % 10 == 9:\n",
    "                print(\"loading time : \" + str(time.time() - loading_time_start))\n",
    "            mini_epoch_start = time.time()\n",
    "            img1, img2, labels = data\n",
    "            reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            value_before = float(list(model.parameters())[-1][0])\n",
    "            # Forward pass\n",
    "            outputs = model(img1, img2)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, reshaped_labels)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            value_after = float(list(model.parameters())[-1][0])\n",
    "            if abs(value_before - value_after) < 0.000000000001:\n",
    "                print(\"VALUES ARE THE SAME\")\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # Print every 10 mini-batches\n",
    "                print('%f per batch : [%d, %5d] loss: %.3f' %\n",
    "                      (time.time() - mini_epoch_start, epoch + 1, i + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "            loading_time_start = time.time()\n",
    "        model.eval()\n",
    "        loss_train, loss_test, acc_train, acc_test = evaluate_model_prep(model, criterion)\n",
    "        print('\\n loss train: %.3f; acc train %.3f \\n loss test %.3f acc test %.3f' %\n",
    "                      (loss_train, acc_train * 100, loss_test, acc_test * 100))\n",
    "        model.train()\n",
    "        epoch_end = time.time()\n",
    "        print(\"EPOCH TIME : \" + str(epoch_end - epoch_start))\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f7a11",
   "metadata": {},
   "source": [
    "Here I restart it and not rerun producing datasets, just using them to train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e11e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, 2, 180, \"processed_dataset/\", 13, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a21399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_metrics(model):\n",
    "    correct_labels = []\n",
    "    predict_labels = []\n",
    "    model.eval()\n",
    "    path = \"processed_eval/\"\n",
    "    max_index = 37\n",
    "    with torch.no_grad():\n",
    "        for i in range (max_index + 1):\n",
    "            with open(path + str(i) + \".pkl\", 'rb') as inp:\n",
    "                data = pickle.load(inp)\n",
    "            img1, img2, labels = data\n",
    "            correct_labels = correct_labels + labels.tolist()\n",
    "            reshaped_labels = torch.reshape(labels, (labels.shape[0], 1)).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(img1, img2)\n",
    "            # Calculate loss\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            predict_labels = predict_labels + predictions.tolist()\n",
    "    print(confusion_matrix(correct_labels, predict_labels))\n",
    "    print('precision %.3f, recall %.3f, f1 %.3f' %\n",
    "              (precision_score(correct_labels, predict_labels), recall_score(correct_labels, predict_labels), f1_score(correct_labels, predict_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions_and_metrics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6e0a5",
   "metadata": {},
   "source": [
    "Ok so I guess it's a bit too prone to predicting positive now, so I'll do the same but \n",
    "1. In different notebook\n",
    "2. a bit more sophisticated NN (+1 layer prob)\n",
    "3. With smaller class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bccfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
